{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchshow as ts\n",
    "from IPython.display import HTML\n",
    "from diffusers import image_processor\n",
    "from diffusers import AutoencoderKL\n",
    "from torchvision import transforms as VT\n",
    "from youtube.data_utils import VideoFilePathToTensor\n",
    "from glob import glob\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from torchvision.io import VideoReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ffmpeg\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_info(filname):\n",
    "    probe = ffmpeg.probe(filname)\n",
    "    video_info = next(s for s in probe[\"streams\"] if s[\"codec_type\"] == \"video\")\n",
    "    info = {\n",
    "        \"nbframes\": video_info[\"nb_frames\"],\n",
    "        \"duration\": video_info[\"duration\"],\n",
    "        \"width\": video_info[\"width\"],\n",
    "        \"height\": video_info[\"height\"],\n",
    "    }\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_video_folder(folder):\n",
    "    # create a csv file with video info\n",
    "    # for each mp4 file in the folder\n",
    "    # get all mp4 files in the folder\n",
    "    data = []\n",
    "    for file in tqdm(glob(folder + \"/*.mp4\")):\n",
    "        # get video info\n",
    "        try:\n",
    "            info = get_video_info(file)\n",
    "            # add to dataframe\n",
    "            filename = file.split(\"/\")[-1]\n",
    "            info_tuple = (\n",
    "                filename,\n",
    "                int(info[\"nbframes\"]),\n",
    "                float(info[\"duration\"]),\n",
    "                info[\"width\"],\n",
    "                info[\"height\"],\n",
    "            )\n",
    "            data.append(info_tuple)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"Error with file: \", file)\n",
    "    df = pd.DataFrame(data, columns=[\"file\", \"nbframes\", \"duration\", \"width\", \"height\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_video_info(\"videos/yxMB0gbW-ak.mp4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = annotate_video_folder(\"dgx_videos\")\n",
    "df = pd.read_csv(\"dgx_videos.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv(\"dgx_videos.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample random based on nbframes score\n",
    "df.sample(10, weights=\"nbframes\", replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import itertools\n",
    "class RandomDataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, annotations, frame_transform=None, video_transform=None, clip_len=16):\n",
    "        super(RandomDataset).__init__()\n",
    "\n",
    "        self.annotations = annotations\n",
    "        self.clip_len = clip_len\n",
    "        self.frame_transform = frame_transform\n",
    "        self.video_transform = video_transform\n",
    "        self.epoch_size = len(self.annotations)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i in range(self.epoch_size):\n",
    "            # Get random sample\n",
    "            sample = self.annotations.sample(1, weights=\"nbframes\", replace=False)\n",
    "            sample = sample.iloc[0]\n",
    "            path = \"dgx_videos/\" + sample[\"file\"]\n",
    "            if not os.path.exists(path):\n",
    "                print(\"path does not exist: \", path)\n",
    "                continue\n",
    "            nbframes = float(sample[\"nbframes\"])\n",
    "            duration = float(sample[\"duration\"])\n",
    "            fps = int(nbframes / duration)\n",
    "            max_seek = duration - (self.clip_len / fps)\n",
    "            vid = VideoReader(path, \"video\")\n",
    "            video_frames = []  # video frame buffer\n",
    "            # Seek and return frames\n",
    "            start = random.uniform(0., max_seek)\n",
    "            for frame in itertools.islice(vid.seek(start), self.clip_len):\n",
    "                data = frame[\"data\"]\n",
    "                if self.frame_transform:\n",
    "                    data = self.frame_transform(data)\n",
    "                video_frames.append(data)\n",
    "                current_pts = frame['pts']\n",
    "            # Stack it into a tensor\n",
    "            video = torch.stack(video_frames, 0) \n",
    "\n",
    "            if self.video_transform:\n",
    "                video = self.video_transform(video)\n",
    "            output = {\n",
    "                'path': path,\n",
    "                'video': video,\n",
    "                'start': start,\n",
    "                'end': current_pts}\n",
    "            yield output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = image_processor.VaeImageProcessor(do_normalize=True)\n",
    "\n",
    "train_transforms = VT.Compose(\n",
    "    [\n",
    "        VT.Resize((256, 512), \n",
    "        interpolation=VT.InterpolationMode.BICUBIC),\n",
    "        #lambda x: x / 255,\n",
    "        #VT.Normalize([0.5], [0.5]),\n",
    "        lambda x: 2 * (x / 255.0) - 1.0\n",
    "    ]\n",
    ")\n",
    "frame_transform = lambda x: x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "dataset = RandomDataset(df, clip_len=1, video_transform=train_transforms, frame_transform=frame_transform)\n",
    "dataloader = DataLoader(dataset, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in dataloader:\n",
    "    print(data['video'].shape)\n",
    "    print(\"min\", data['video'].min(), \"max\", data['video'].max())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class VAELoss(nn.Module):\n",
    "    def __init__(self, \n",
    "                 logvar_init=0.0, \n",
    "                 kl_weight=1.0, \n",
    "                 pixelloss_weight=1.0):\n",
    "\n",
    "        super().__init__()\n",
    "        self.kl_weight = kl_weight\n",
    "        self.pixel_weight = pixelloss_weight\n",
    "        # output log variance\n",
    "        #self.logvar = nn.Parameter(torch.ones(size=()) * logvar_init)\n",
    "\n",
    "\n",
    "    def forward(self, \n",
    "                inputs, \n",
    "                reconstructions, \n",
    "                posteriors):\n",
    "        rec_loss = torch.abs(inputs.contiguous() - reconstructions.contiguous())\n",
    "        #nll_loss = rec_loss / torch.exp(self.logvar) + self.logvar\n",
    "        nll_loss = rec_loss\n",
    "        weighted_nll_loss = nll_loss\n",
    "        weighted_nll_loss = torch.sum(weighted_nll_loss) / weighted_nll_loss.shape[0]\n",
    "        nll_loss = torch.sum(nll_loss) / nll_loss.shape[0]\n",
    "        kl_loss = posteriors.kl()\n",
    "        kl_loss = torch.sum(kl_loss) / kl_loss.shape[0]\n",
    "        loss = kl_loss * self.kl_weight + weighted_nll_loss * self.pixel_weight\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoencoderKLWLoss(nn.Module):\n",
    "    def __init__(self, vae: AutoencoderKL, kl_weight=1.0, pixelloss_weight=1.0):\n",
    "        super().__init__()\n",
    "        self.vae = vae\n",
    "        self.loss = VAELoss(kl_weight=kl_weight, pixelloss_weight=pixelloss_weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        posteriors = self.encode(x)\n",
    "        z = posteriors.sample()\n",
    "        reconstructions = self.decode(z)\n",
    "        return reconstructions, posteriors\n",
    "    \n",
    "    def encode(self, x):\n",
    "        return self.vae.encode(x).latent_dist\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.vae.decode(z).sample\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = AutoencoderKL.from_pretrained(\n",
    "    #\"runwayml/stable-diffusion-v1-4\",\n",
    "    #\"stabilityai/sd-vae-ft-mse\",\n",
    "    \"stabilityai/stable-diffusion-2-1\",\n",
    "    #\"stabilityai/sd-vae-ft-ema-original\",\n",
    "    #\"CompVis/stable-diffusion-v1-4\",\n",
    "    subfolder=\"vae\",\n",
    ")\n",
    "\n",
    "vae = AutoencoderKLWLoss(vae, kl_weight=1.0, pixelloss_weight=1.0).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['video'][0].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "sum([math.prod(p.shape) for p in vae.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE Training LOOP\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.nn import MSELoss\n",
    "from diffusers.training_utils import EMAModel\n",
    "\n",
    "def train(model, \n",
    "          df, \n",
    "          batch_size: int = 4, \n",
    "          gaccum: int = 8,\n",
    "          total_iter: int = 100,\n",
    "          ):\n",
    "     \n",
    "    train_transforms = VT.Compose(\n",
    "        [\n",
    "            VT.Resize((256, 512), \n",
    "            interpolation=VT.InterpolationMode.BICUBIC),\n",
    "            lambda x: 2 * (x / 255.0) - 1.0\n",
    "        ]\n",
    "    )\n",
    "    frame_transform = lambda x: x \n",
    "    dataset = RandomDataset(df, \n",
    "                            clip_len=1, \n",
    "                            video_transform=train_transforms, \n",
    "                            frame_transform=frame_transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "    optim = AdamW(model.parameters(), lr=1e-4)\n",
    "    scheduler = CosineAnnealingLR(optim, total_iter, eta_min=1e-6)\n",
    "    n_iter = 0\n",
    "    total_loss = 0.0\n",
    "    n_forward = 0\n",
    "    model.train()\n",
    "    while n_iter < total_iter:\n",
    "        for data in  tqdm(dataloader):\n",
    "            video = data['video'].squeeze(1)\n",
    "            video = video.cuda()\n",
    "            reconstructions, posteriors = model(video)\n",
    "            loss = model.loss(video, reconstructions, posteriors)\n",
    "            loss.backward()\n",
    "            total_loss += loss.item()\n",
    "            n_forward += 1\n",
    "            if gaccum % batch_size == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 10.0)\n",
    "                optim.step()\n",
    "                optim.zero_grad()\n",
    "                scheduler.step()\n",
    "                n_iter += 1\n",
    "                if n_iter >= total_iter:\n",
    "                   break\n",
    "                if n_iter % 100 == 0:\n",
    "                    print(\"Iter: \", n_iter, \"Loss: \", total_loss / n_forward)\n",
    "                    total_loss = 0.0\n",
    "                    n_forward = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(vae, df, batch_size=4, gaccum=16, total_iter=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = torch.load(\"vae.pt\")\n",
    "vae.cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = RandomDataset(df, clip_len=1, video_transform=train_transforms, frame_transform=frame_transform)\n",
    "dataloader = DataLoader(dataset, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original = data['video'][:,0].cuda()\n",
    "encodings = vae.encode(original)\n",
    "z = encodings.sample()\n",
    "recon = vae.decode(z)\n",
    "recon = recon.clamp(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recon.min(), recon.max(), original.min(),   original.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.show(original[1].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.show(recon[1].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = AutoencoderKL.from_pretrained(\n",
    "    #\"runwayml/stable-diffusion-v1-4\",\n",
    "    #\"stabilityai/sd-vae-ft-mse\",\n",
    "    \"stabilityai/stable-diffusion-2-1\",\n",
    "    #\"stabilityai/sd-vae-ft-ema-original\",\n",
    "    #\"CompVis/stable-diffusion-v1-4\",\n",
    "    subfolder=\"vae\",\n",
    ")\n",
    "\n",
    "vae = AutoencoderKLWLoss(vae, kl_weight=1.0, pixelloss_weight=1.0).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original = data['video'][:,0].cuda()\n",
    "encodings = vae.encode(original)\n",
    "z = encodings.sample()\n",
    "recon = vae.decode(z)\n",
    "recon = recon.clamp(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.show(recon[1].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minedojo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
